{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramsha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import scipy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit, ParameterGrid\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_train = shuffle(pd.read_csv(\"../data/yelp/yelp-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label']))\n",
    "yelp_valid = shuffle(pd.read_csv(\"../data/yelp/yelp-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label']))\n",
    "yelp_test = shuffle(pd.read_csv(\"../data/yelp/yelp-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label']))\n",
    "\n",
    "IMDB_train = shuffle(pd.read_csv(\"../data/IMDB/IMDB-train.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label']))\n",
    "IMDB_valid = shuffle(pd.read_csv(\"../data/IMDB/IMDB-valid.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label']))\n",
    "IMDB_test = shuffle(pd.read_csv(\"../data/IMDB/IMDB-test.txt\", sep='\\t', lineterminator='\\n', header=None, names=['review', 'label']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 review  label\n",
      "3320  I always just get one thing from Vietnamese pl...      3\n",
      "460   I really like eating here.  I have only been f...      4\n",
      "3453  I decided to break from routine and get a hair...      1\n",
      "6718  MARGARITA time....I love their ritas on the ro...      5\n",
      "1118  My wife and I have going to this restaurant on...      1\n"
     ]
    }
   ],
   "source": [
    "print(yelp_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'yelp': {'train': yelp_train, 'valid': yelp_valid, 'test': yelp_test},\n",
    "    'IMDB': {'train': IMDB_train, 'valid': IMDB_valid, 'test': IMDB_test},\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets.values():\n",
    "    for split in dataset.values():\n",
    "        split['review'] = split['review'].str.replace('<br /><br />', ' ').str.replace('[^\\w\\s]', '').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "MAX_FEATURES = 10000\n",
    "vocabulary = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    # accumulate all words\n",
    "    all_words = [word for sentence in dataset['train']['review'].str.split().tolist() for word in sentence]\n",
    "    # keep most frequent words, exclude stop words\n",
    "    freq_words = Counter(word for word in all_words if word not in stop_words).most_common(MAX_FEATURES)\n",
    "    # create dictionary for most frequent words\n",
    "    vocabulary[dataset_name] = {word[0]: i for i, word in enumerate(freq_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BOW(dataset, vocabulary, x_name='review', y_name='label'):\n",
    "    # convert each split (train/valid/test) for each dataset (yelp/IMDB) to BoW representations\n",
    "    BBOW = {}\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    for split_name, split in dataset.items():\n",
    "        vector = vectorizer.fit_transform(split[x_name])\n",
    "        vector[vector > 1] = 1\n",
    "        BBOW[split_name] = [vector, split[y_name]]\n",
    "    return BBOW\n",
    "\n",
    "yelp_BBOW= get_BOW(datasets['yelp'], vocabulary['yelp'])\n",
    "IMDB_BBOW = get_BOW(datasets['IMDB'], vocabulary['IMDB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \n",
    "    'Logistic Regression': {\n",
    "        'classifer' : LogisticRegression(),\n",
    "         'param' : {\n",
    "            'tol': np.arange(0.0005, 0.001, .0011),\n",
    "            'C': np.arange(0.01, 1.01, 0.01)\n",
    "            }\n",
    "            \n",
    "        },\n",
    "    'Bernoulli Naive Bayes': {\n",
    "        'classifer' : BernoulliNB(),\n",
    "        'param' : {\n",
    "            'alpha': np.arange(0.01, 1.01, 0.01)\n",
    "        }\n",
    "    },\n",
    "   \n",
    "    'Decision Trees': {\n",
    "        'classifer' : DecisionTreeClassifier(),\n",
    "        'param' : {\n",
    "            'max_depth': np.arange(13, 17),\n",
    "            'max_features': np.arange(0.1, 0.5, 0.1),\n",
    "            'min_samples_leaf': np.arange(3, 6)\n",
    "        },\n",
    "    },\n",
    "    'Linear SVM': {\n",
    "        'classifer' : LinearSVC(),\n",
    "        'param' : {\n",
    "            'C': np.logspace(-2, 2, num=8),\n",
    "            'max_iter': np.arange(1000, 2000, 100)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(models, model, dataset_BOW, tune=False, average='micro'):\n",
    "    # assign classifier and hyperparameters\n",
    "    classifier = models[model]['classifer']\n",
    "    param = models[model]['param']\n",
    "    # tune hyperparameters\n",
    "    train_x = dataset_BOW['train'][0]\n",
    "    train_y = dataset_BOW['train'][1]\n",
    "    if tune and param is not None:\n",
    "        valid_x = dataset_BOW['valid'][0]\n",
    "        valid_y = dataset_BOW['valid'][1]\n",
    "        ps = PredefinedSplit(test_fold=[-1 if i < len(train_y) else 0 for i in range(len(train_y) + len(valid_y))])\n",
    "        classifier = GridSearchCV(classifier, param, cv=ps, scoring='f1_micro', n_jobs=2, verbose=10)\n",
    "        train_x = scipy.sparse.vstack([train_x, valid_x])\n",
    "        train_y = np.concatenate([train_y, valid_y])                                          \n",
    "    # fit model\n",
    "    classifier.fit(train_x, train_y)\n",
    "    # predict and compute f1 score for every split (train/valid/test)\n",
    "    print(f\"{model.upper()}\\n\")\n",
    "    if tune and param is not None:\n",
    "        print(f\"Hypertuning Parameters:\")\n",
    "        means = classifier.cv_results_['mean_test_score']\n",
    "        for mean, params in zip(means, classifier.cv_results_['params']):\n",
    "            print(\"%0.4f for %r\" % (mean, params))\n",
    "        print(f\"\\nOptimal Parameters: {classifier.best_params_}\")\n",
    "    print(f\"Corresponding F1 Scores:\")\n",
    "    for split_name, split in dataset_BOW.items():\n",
    "        y_true = split[1]\n",
    "        y_pred = classifier.predict(split[0])\n",
    "        f1 = f1_score(y_true, y_pred, average=average)   \n",
    "        #print(f\"{split_name.upper()}: {f1}\")\n",
    "        print(\"%s: %0.4f\" % (split_name.upper(), f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   22.7s\n",
      "[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=2)]: Done  81 tasks      | elapsed:   46.7s\n",
      "[Parallel(n_jobs=2)]: Done  94 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:  1.0min finished\n",
      "C:\\Users\\ramsha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\ramsha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.4650 for {'C': 0.01, 'tol': 0.0005}\n",
      "0.4900 for {'C': 0.02, 'tol': 0.0005}\n",
      "0.4900 for {'C': 0.03, 'tol': 0.0005}\n",
      "0.4890 for {'C': 0.04, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.05, 'tol': 0.0005}\n",
      "0.4880 for {'C': 0.060000000000000005, 'tol': 0.0005}\n",
      "0.4870 for {'C': 0.06999999999999999, 'tol': 0.0005}\n",
      "0.4850 for {'C': 0.08, 'tol': 0.0005}\n",
      "0.4860 for {'C': 0.09, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.09999999999999999, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.11, 'tol': 0.0005}\n",
      "0.4880 for {'C': 0.12, 'tol': 0.0005}\n",
      "0.4900 for {'C': 0.13, 'tol': 0.0005}\n",
      "0.4890 for {'C': 0.14, 'tol': 0.0005}\n",
      "0.4860 for {'C': 0.15000000000000002, 'tol': 0.0005}\n",
      "0.4850 for {'C': 0.16, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.17, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.18000000000000002, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.19, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.2, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.21000000000000002, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.22, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.23, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.24000000000000002, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.25, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.26, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.27, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.28, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.29000000000000004, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.3, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.31, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.32, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.33, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.34, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.35000000000000003, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.36000000000000004, 'tol': 0.0005}\n",
      "0.4840 for {'C': 0.37, 'tol': 0.0005}\n",
      "0.4850 for {'C': 0.38, 'tol': 0.0005}\n",
      "0.4830 for {'C': 0.39, 'tol': 0.0005}\n",
      "0.4820 for {'C': 0.4, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.41000000000000003, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.42000000000000004, 'tol': 0.0005}\n",
      "0.4800 for {'C': 0.43, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.44, 'tol': 0.0005}\n",
      "0.4810 for {'C': 0.45, 'tol': 0.0005}\n",
      "0.4800 for {'C': 0.46, 'tol': 0.0005}\n",
      "0.4800 for {'C': 0.47000000000000003, 'tol': 0.0005}\n",
      "0.4780 for {'C': 0.48000000000000004, 'tol': 0.0005}\n",
      "0.4800 for {'C': 0.49, 'tol': 0.0005}\n",
      "0.4790 for {'C': 0.5, 'tol': 0.0005}\n",
      "0.4770 for {'C': 0.51, 'tol': 0.0005}\n",
      "0.4770 for {'C': 0.52, 'tol': 0.0005}\n",
      "0.4770 for {'C': 0.53, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.54, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.55, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.56, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.5700000000000001, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.5800000000000001, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.59, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.6, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.61, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.62, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.63, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.64, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.65, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.66, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.67, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.68, 'tol': 0.0005}\n",
      "0.4770 for {'C': 0.6900000000000001, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.7000000000000001, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.7100000000000001, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.72, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.73, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.74, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.75, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.76, 'tol': 0.0005}\n",
      "0.4760 for {'C': 0.77, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.78, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.79, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.8, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.81, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.8200000000000001, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.8300000000000001, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.8400000000000001, 'tol': 0.0005}\n",
      "0.4750 for {'C': 0.85, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.86, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.87, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.88, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.89, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.9, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.91, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.92, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.93, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.9400000000000001, 'tol': 0.0005}\n",
      "0.4730 for {'C': 0.9500000000000001, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.9600000000000001, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.97, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.98, 'tol': 0.0005}\n",
      "0.4740 for {'C': 0.99, 'tol': 0.0005}\n",
      "0.4750 for {'C': 1.0, 'tol': 0.0005}\n",
      "\n",
      "Optimal Parameters: {'C': 0.02, 'tol': 0.0005}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.6793\n",
      "VALID: 0.6640\n",
      "TEST: 0.5160\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Logistic Regression', yelp_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Batch computation too fast (0.0839s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=2)]: Done  44 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=2)]: Done  72 tasks      | elapsed:    1.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERNOULLI NAIVE BAYES\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.4290 for {'alpha': 0.01}\n",
      "0.4330 for {'alpha': 0.02}\n",
      "0.4370 for {'alpha': 0.03}\n",
      "0.4340 for {'alpha': 0.04}\n",
      "0.4320 for {'alpha': 0.05}\n",
      "0.4330 for {'alpha': 0.060000000000000005}\n",
      "0.4330 for {'alpha': 0.06999999999999999}\n",
      "0.4300 for {'alpha': 0.08}\n",
      "0.4260 for {'alpha': 0.09}\n",
      "0.4260 for {'alpha': 0.09999999999999999}\n",
      "0.4220 for {'alpha': 0.11}\n",
      "0.4210 for {'alpha': 0.12}\n",
      "0.4170 for {'alpha': 0.13}\n",
      "0.4150 for {'alpha': 0.14}\n",
      "0.4150 for {'alpha': 0.15000000000000002}\n",
      "0.4140 for {'alpha': 0.16}\n",
      "0.4140 for {'alpha': 0.17}\n",
      "0.4140 for {'alpha': 0.18000000000000002}\n",
      "0.4140 for {'alpha': 0.19}\n",
      "0.4140 for {'alpha': 0.2}\n",
      "0.4150 for {'alpha': 0.21000000000000002}\n",
      "0.4150 for {'alpha': 0.22}\n",
      "0.4130 for {'alpha': 0.23}\n",
      "0.4130 for {'alpha': 0.24000000000000002}\n",
      "0.4130 for {'alpha': 0.25}\n",
      "0.4150 for {'alpha': 0.26}\n",
      "0.4140 for {'alpha': 0.27}\n",
      "0.4140 for {'alpha': 0.28}\n",
      "0.4150 for {'alpha': 0.29000000000000004}\n",
      "0.4130 for {'alpha': 0.3}\n",
      "0.4110 for {'alpha': 0.31}\n",
      "0.4140 for {'alpha': 0.32}\n",
      "0.4120 for {'alpha': 0.33}\n",
      "0.4120 for {'alpha': 0.34}\n",
      "0.4130 for {'alpha': 0.35000000000000003}\n",
      "0.4120 for {'alpha': 0.36000000000000004}\n",
      "0.4130 for {'alpha': 0.37}\n",
      "0.4130 for {'alpha': 0.38}\n",
      "0.4110 for {'alpha': 0.39}\n",
      "0.4100 for {'alpha': 0.4}\n",
      "0.4100 for {'alpha': 0.41000000000000003}\n",
      "0.4090 for {'alpha': 0.42000000000000004}\n",
      "0.4080 for {'alpha': 0.43}\n",
      "0.4080 for {'alpha': 0.44}\n",
      "0.4060 for {'alpha': 0.45}\n",
      "0.4040 for {'alpha': 0.46}\n",
      "0.4030 for {'alpha': 0.47000000000000003}\n",
      "0.4030 for {'alpha': 0.48000000000000004}\n",
      "0.4030 for {'alpha': 0.49}\n",
      "0.4030 for {'alpha': 0.5}\n",
      "0.4020 for {'alpha': 0.51}\n",
      "0.4020 for {'alpha': 0.52}\n",
      "0.4020 for {'alpha': 0.53}\n",
      "0.4020 for {'alpha': 0.54}\n",
      "0.4030 for {'alpha': 0.55}\n",
      "0.4030 for {'alpha': 0.56}\n",
      "0.4020 for {'alpha': 0.5700000000000001}\n",
      "0.4030 for {'alpha': 0.5800000000000001}\n",
      "0.4040 for {'alpha': 0.59}\n",
      "0.4030 for {'alpha': 0.6}\n",
      "0.4010 for {'alpha': 0.61}\n",
      "0.4000 for {'alpha': 0.62}\n",
      "0.4000 for {'alpha': 0.63}\n",
      "0.4040 for {'alpha': 0.64}\n",
      "0.4030 for {'alpha': 0.65}\n",
      "0.4030 for {'alpha': 0.66}\n",
      "0.4030 for {'alpha': 0.67}\n",
      "0.4040 for {'alpha': 0.68}\n",
      "0.4040 for {'alpha': 0.6900000000000001}\n",
      "0.4040 for {'alpha': 0.7000000000000001}\n",
      "0.4040 for {'alpha': 0.7100000000000001}\n",
      "0.4050 for {'alpha': 0.72}\n",
      "0.4060 for {'alpha': 0.73}\n",
      "0.4040 for {'alpha': 0.74}\n",
      "0.4030 for {'alpha': 0.75}\n",
      "0.4040 for {'alpha': 0.76}\n",
      "0.4030 for {'alpha': 0.77}\n",
      "0.4030 for {'alpha': 0.78}\n",
      "0.4020 for {'alpha': 0.79}\n",
      "0.4030 for {'alpha': 0.8}\n",
      "0.4030 for {'alpha': 0.81}\n",
      "0.4030 for {'alpha': 0.8200000000000001}\n",
      "0.4040 for {'alpha': 0.8300000000000001}\n",
      "0.4060 for {'alpha': 0.8400000000000001}\n",
      "0.4070 for {'alpha': 0.85}\n",
      "0.4060 for {'alpha': 0.86}\n",
      "0.4050 for {'alpha': 0.87}\n",
      "0.4040 for {'alpha': 0.88}\n",
      "0.4020 for {'alpha': 0.89}\n",
      "0.4020 for {'alpha': 0.9}\n",
      "0.4010 for {'alpha': 0.91}\n",
      "0.4010 for {'alpha': 0.92}\n",
      "0.4010 for {'alpha': 0.93}\n",
      "0.4000 for {'alpha': 0.9400000000000001}\n",
      "0.4010 for {'alpha': 0.9500000000000001}\n",
      "0.4000 for {'alpha': 0.9600000000000001}\n",
      "0.4000 for {'alpha': 0.97}\n",
      "0.3970 for {'alpha': 0.98}\n",
      "0.3970 for {'alpha': 0.99}\n",
      "0.3960 for {'alpha': 1.0}\n",
      "\n",
      "Optimal Parameters: {'alpha': 0.03}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.7573\n",
      "VALID: 0.7310\n",
      "TEST: 0.4520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Bernoulli Naive Bayes', yelp_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 48 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=2)]: Done  48 out of  48 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREES\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.3620 for {'max_depth': 13, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.4020 for {'max_depth': 13, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.3970 for {'max_depth': 13, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.3860 for {'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.3970 for {'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.3810 for {'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.3800 for {'max_depth': 13, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.3970 for {'max_depth': 13, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.3640 for {'max_depth': 13, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.4100 for {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.3780 for {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.3780 for {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "0.3730 for {'max_depth': 14, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.3760 for {'max_depth': 14, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.3710 for {'max_depth': 14, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.3860 for {'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.3770 for {'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.3950 for {'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.3800 for {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.4080 for {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.3880 for {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.4010 for {'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.4060 for {'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.3850 for {'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "0.3790 for {'max_depth': 15, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.3920 for {'max_depth': 15, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.3730 for {'max_depth': 15, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.3650 for {'max_depth': 15, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.3650 for {'max_depth': 15, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.3900 for {'max_depth': 15, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.3680 for {'max_depth': 15, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.3940 for {'max_depth': 15, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.3700 for {'max_depth': 15, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.3770 for {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.4000 for {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.3800 for {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "0.3760 for {'max_depth': 16, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.3870 for {'max_depth': 16, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.3800 for {'max_depth': 16, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.3980 for {'max_depth': 16, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.3790 for {'max_depth': 16, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.3860 for {'max_depth': 16, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.3980 for {'max_depth': 16, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.3800 for {'max_depth': 16, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.3910 for {'max_depth': 16, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.3750 for {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.3930 for {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.3670 for {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "\n",
      "Optimal Parameters: {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.5306\n",
      "VALID: 0.5310\n",
      "TEST: 0.4010\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Decision Trees', yelp_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 80 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed:   40.3s\n",
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=2)]: Done  80 out of  80 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR SVM\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1000}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1100}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1200}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1300}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1400}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1500}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1600}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1700}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1800}\n",
      "0.4810 for {'C': 0.01, 'max_iter': 1900}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1000}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1100}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1200}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1300}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1400}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1500}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1600}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1700}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1800}\n",
      "0.4680 for {'C': 0.0372759372031494, 'max_iter': 1900}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1000}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1100}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1200}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1300}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1400}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1500}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1600}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1700}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1800}\n",
      "0.4710 for {'C': 0.13894954943731375, 'max_iter': 1900}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1000}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1100}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1200}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1300}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1400}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1500}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1600}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1700}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1800}\n",
      "0.4470 for {'C': 0.517947467923121, 'max_iter': 1900}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1000}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1100}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1200}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1300}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1400}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1500}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1600}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1700}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1800}\n",
      "0.4350 for {'C': 1.9306977288832496, 'max_iter': 1900}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1000}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1100}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1200}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1300}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1400}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1500}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1600}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1700}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1800}\n",
      "0.4320 for {'C': 7.196856730011514, 'max_iter': 1900}\n",
      "0.4320 for {'C': 26.826957952797247, 'max_iter': 1000}\n",
      "0.4300 for {'C': 26.826957952797247, 'max_iter': 1100}\n",
      "0.4310 for {'C': 26.826957952797247, 'max_iter': 1200}\n",
      "0.4300 for {'C': 26.826957952797247, 'max_iter': 1300}\n",
      "0.4300 for {'C': 26.826957952797247, 'max_iter': 1400}\n",
      "0.4320 for {'C': 26.826957952797247, 'max_iter': 1500}\n",
      "0.4320 for {'C': 26.826957952797247, 'max_iter': 1600}\n",
      "0.4310 for {'C': 26.826957952797247, 'max_iter': 1700}\n",
      "0.4310 for {'C': 26.826957952797247, 'max_iter': 1800}\n",
      "0.4310 for {'C': 26.826957952797247, 'max_iter': 1900}\n",
      "0.4340 for {'C': 100.0, 'max_iter': 1000}\n",
      "0.4350 for {'C': 100.0, 'max_iter': 1100}\n",
      "0.4330 for {'C': 100.0, 'max_iter': 1200}\n",
      "0.4350 for {'C': 100.0, 'max_iter': 1300}\n",
      "0.4320 for {'C': 100.0, 'max_iter': 1400}\n",
      "0.4340 for {'C': 100.0, 'max_iter': 1500}\n",
      "0.4360 for {'C': 100.0, 'max_iter': 1600}\n",
      "0.4340 for {'C': 100.0, 'max_iter': 1700}\n",
      "0.4370 for {'C': 100.0, 'max_iter': 1800}\n",
      "0.4350 for {'C': 100.0, 'max_iter': 1900}\n",
      "\n",
      "Optimal Parameters: {'C': 0.01, 'max_iter': 1000}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.8214\n",
      "VALID: 0.8330\n",
      "TEST: 0.5070\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Linear SVM', yelp_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   19.8s\n",
      "[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=2)]: Done  81 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=2)]: Done  94 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:   47.5s finished\n",
      "C:\\Users\\ramsha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.8700 for {'C': 0.01, 'tol': 0.0005}\n",
      "0.8757 for {'C': 0.02, 'tol': 0.0005}\n",
      "0.8774 for {'C': 0.03, 'tol': 0.0005}\n",
      "0.8783 for {'C': 0.04, 'tol': 0.0005}\n",
      "0.8795 for {'C': 0.05, 'tol': 0.0005}\n",
      "0.8792 for {'C': 0.060000000000000005, 'tol': 0.0005}\n",
      "0.8794 for {'C': 0.06999999999999999, 'tol': 0.0005}\n",
      "0.8803 for {'C': 0.08, 'tol': 0.0005}\n",
      "0.8797 for {'C': 0.09, 'tol': 0.0005}\n",
      "0.8800 for {'C': 0.09999999999999999, 'tol': 0.0005}\n",
      "0.8791 for {'C': 0.11, 'tol': 0.0005}\n",
      "0.8785 for {'C': 0.12, 'tol': 0.0005}\n",
      "0.8780 for {'C': 0.13, 'tol': 0.0005}\n",
      "0.8776 for {'C': 0.14, 'tol': 0.0005}\n",
      "0.8779 for {'C': 0.15000000000000002, 'tol': 0.0005}\n",
      "0.8772 for {'C': 0.16, 'tol': 0.0005}\n",
      "0.8770 for {'C': 0.17, 'tol': 0.0005}\n",
      "0.8775 for {'C': 0.18000000000000002, 'tol': 0.0005}\n",
      "0.8771 for {'C': 0.19, 'tol': 0.0005}\n",
      "0.8768 for {'C': 0.2, 'tol': 0.0005}\n",
      "0.8768 for {'C': 0.21000000000000002, 'tol': 0.0005}\n",
      "0.8761 for {'C': 0.22, 'tol': 0.0005}\n",
      "0.8755 for {'C': 0.23, 'tol': 0.0005}\n",
      "0.8754 for {'C': 0.24000000000000002, 'tol': 0.0005}\n",
      "0.8749 for {'C': 0.25, 'tol': 0.0005}\n",
      "0.8750 for {'C': 0.26, 'tol': 0.0005}\n",
      "0.8749 for {'C': 0.27, 'tol': 0.0005}\n",
      "0.8753 for {'C': 0.28, 'tol': 0.0005}\n",
      "0.8748 for {'C': 0.29000000000000004, 'tol': 0.0005}\n",
      "0.8747 for {'C': 0.3, 'tol': 0.0005}\n",
      "0.8742 for {'C': 0.31, 'tol': 0.0005}\n",
      "0.8742 for {'C': 0.32, 'tol': 0.0005}\n",
      "0.8741 for {'C': 0.33, 'tol': 0.0005}\n",
      "0.8734 for {'C': 0.34, 'tol': 0.0005}\n",
      "0.8730 for {'C': 0.35000000000000003, 'tol': 0.0005}\n",
      "0.8728 for {'C': 0.36000000000000004, 'tol': 0.0005}\n",
      "0.8723 for {'C': 0.37, 'tol': 0.0005}\n",
      "0.8722 for {'C': 0.38, 'tol': 0.0005}\n",
      "0.8721 for {'C': 0.39, 'tol': 0.0005}\n",
      "0.8712 for {'C': 0.4, 'tol': 0.0005}\n",
      "0.8710 for {'C': 0.41000000000000003, 'tol': 0.0005}\n",
      "0.8707 for {'C': 0.42000000000000004, 'tol': 0.0005}\n",
      "0.8709 for {'C': 0.43, 'tol': 0.0005}\n",
      "0.8709 for {'C': 0.44, 'tol': 0.0005}\n",
      "0.8708 for {'C': 0.45, 'tol': 0.0005}\n",
      "0.8708 for {'C': 0.46, 'tol': 0.0005}\n",
      "0.8706 for {'C': 0.47000000000000003, 'tol': 0.0005}\n",
      "0.8703 for {'C': 0.48000000000000004, 'tol': 0.0005}\n",
      "0.8700 for {'C': 0.49, 'tol': 0.0005}\n",
      "0.8701 for {'C': 0.5, 'tol': 0.0005}\n",
      "0.8700 for {'C': 0.51, 'tol': 0.0005}\n",
      "0.8701 for {'C': 0.52, 'tol': 0.0005}\n",
      "0.8700 for {'C': 0.53, 'tol': 0.0005}\n",
      "0.8700 for {'C': 0.54, 'tol': 0.0005}\n",
      "0.8697 for {'C': 0.55, 'tol': 0.0005}\n",
      "0.8698 for {'C': 0.56, 'tol': 0.0005}\n",
      "0.8696 for {'C': 0.5700000000000001, 'tol': 0.0005}\n",
      "0.8698 for {'C': 0.5800000000000001, 'tol': 0.0005}\n",
      "0.8698 for {'C': 0.59, 'tol': 0.0005}\n",
      "0.8699 for {'C': 0.6, 'tol': 0.0005}\n",
      "0.8696 for {'C': 0.61, 'tol': 0.0005}\n",
      "0.8696 for {'C': 0.62, 'tol': 0.0005}\n",
      "0.8694 for {'C': 0.63, 'tol': 0.0005}\n",
      "0.8694 for {'C': 0.64, 'tol': 0.0005}\n",
      "0.8694 for {'C': 0.65, 'tol': 0.0005}\n",
      "0.8694 for {'C': 0.66, 'tol': 0.0005}\n",
      "0.8691 for {'C': 0.67, 'tol': 0.0005}\n",
      "0.8691 for {'C': 0.68, 'tol': 0.0005}\n",
      "0.8688 for {'C': 0.6900000000000001, 'tol': 0.0005}\n",
      "0.8688 for {'C': 0.7000000000000001, 'tol': 0.0005}\n",
      "0.8684 for {'C': 0.7100000000000001, 'tol': 0.0005}\n",
      "0.8683 for {'C': 0.72, 'tol': 0.0005}\n",
      "0.8683 for {'C': 0.73, 'tol': 0.0005}\n",
      "0.8681 for {'C': 0.74, 'tol': 0.0005}\n",
      "0.8677 for {'C': 0.75, 'tol': 0.0005}\n",
      "0.8677 for {'C': 0.76, 'tol': 0.0005}\n",
      "0.8676 for {'C': 0.77, 'tol': 0.0005}\n",
      "0.8674 for {'C': 0.78, 'tol': 0.0005}\n",
      "0.8674 for {'C': 0.79, 'tol': 0.0005}\n",
      "0.8677 for {'C': 0.8, 'tol': 0.0005}\n",
      "0.8675 for {'C': 0.81, 'tol': 0.0005}\n",
      "0.8675 for {'C': 0.8200000000000001, 'tol': 0.0005}\n",
      "0.8675 for {'C': 0.8300000000000001, 'tol': 0.0005}\n",
      "0.8674 for {'C': 0.8400000000000001, 'tol': 0.0005}\n",
      "0.8675 for {'C': 0.85, 'tol': 0.0005}\n",
      "0.8671 for {'C': 0.86, 'tol': 0.0005}\n",
      "0.8671 for {'C': 0.87, 'tol': 0.0005}\n",
      "0.8671 for {'C': 0.88, 'tol': 0.0005}\n",
      "0.8669 for {'C': 0.89, 'tol': 0.0005}\n",
      "0.8668 for {'C': 0.9, 'tol': 0.0005}\n",
      "0.8663 for {'C': 0.91, 'tol': 0.0005}\n",
      "0.8664 for {'C': 0.92, 'tol': 0.0005}\n",
      "0.8662 for {'C': 0.93, 'tol': 0.0005}\n",
      "0.8661 for {'C': 0.9400000000000001, 'tol': 0.0005}\n",
      "0.8661 for {'C': 0.9500000000000001, 'tol': 0.0005}\n",
      "0.8661 for {'C': 0.9600000000000001, 'tol': 0.0005}\n",
      "0.8662 for {'C': 0.97, 'tol': 0.0005}\n",
      "0.8661 for {'C': 0.98, 'tol': 0.0005}\n",
      "0.8659 for {'C': 0.99, 'tol': 0.0005}\n",
      "0.8660 for {'C': 1.0, 'tol': 0.0005}\n",
      "\n",
      "Optimal Parameters: {'C': 0.08, 'tol': 0.0005}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.9461\n",
      "VALID: 0.9446\n",
      "TEST: 0.8786\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Logistic Regression', IMDB_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 100 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=2)]: Done  81 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=2)]: Done  94 tasks      | elapsed:    7.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERNOULLI NAIVE BAYES\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.8463 for {'alpha': 0.01}\n",
      "0.8463 for {'alpha': 0.02}\n",
      "0.8463 for {'alpha': 0.03}\n",
      "0.8465 for {'alpha': 0.04}\n",
      "0.8465 for {'alpha': 0.05}\n",
      "0.8466 for {'alpha': 0.060000000000000005}\n",
      "0.8465 for {'alpha': 0.06999999999999999}\n",
      "0.8465 for {'alpha': 0.08}\n",
      "0.8465 for {'alpha': 0.09}\n",
      "0.8464 for {'alpha': 0.09999999999999999}\n",
      "0.8464 for {'alpha': 0.11}\n",
      "0.8464 for {'alpha': 0.12}\n",
      "0.8465 for {'alpha': 0.13}\n",
      "0.8465 for {'alpha': 0.14}\n",
      "0.8465 for {'alpha': 0.15000000000000002}\n",
      "0.8465 for {'alpha': 0.16}\n",
      "0.8464 for {'alpha': 0.17}\n",
      "0.8461 for {'alpha': 0.18000000000000002}\n",
      "0.8461 for {'alpha': 0.19}\n",
      "0.8461 for {'alpha': 0.2}\n",
      "0.8461 for {'alpha': 0.21000000000000002}\n",
      "0.8461 for {'alpha': 0.22}\n",
      "0.8460 for {'alpha': 0.23}\n",
      "0.8461 for {'alpha': 0.24000000000000002}\n",
      "0.8461 for {'alpha': 0.25}\n",
      "0.8461 for {'alpha': 0.26}\n",
      "0.8461 for {'alpha': 0.27}\n",
      "0.8461 for {'alpha': 0.28}\n",
      "0.8460 for {'alpha': 0.29000000000000004}\n",
      "0.8461 for {'alpha': 0.3}\n",
      "0.8459 for {'alpha': 0.31}\n",
      "0.8459 for {'alpha': 0.32}\n",
      "0.8458 for {'alpha': 0.33}\n",
      "0.8457 for {'alpha': 0.34}\n",
      "0.8457 for {'alpha': 0.35000000000000003}\n",
      "0.8456 for {'alpha': 0.36000000000000004}\n",
      "0.8457 for {'alpha': 0.37}\n",
      "0.8457 for {'alpha': 0.38}\n",
      "0.8457 for {'alpha': 0.39}\n",
      "0.8455 for {'alpha': 0.4}\n",
      "0.8455 for {'alpha': 0.41000000000000003}\n",
      "0.8455 for {'alpha': 0.42000000000000004}\n",
      "0.8456 for {'alpha': 0.43}\n",
      "0.8457 for {'alpha': 0.44}\n",
      "0.8458 for {'alpha': 0.45}\n",
      "0.8459 for {'alpha': 0.46}\n",
      "0.8458 for {'alpha': 0.47000000000000003}\n",
      "0.8457 for {'alpha': 0.48000000000000004}\n",
      "0.8457 for {'alpha': 0.49}\n",
      "0.8457 for {'alpha': 0.5}\n",
      "0.8457 for {'alpha': 0.51}\n",
      "0.8456 for {'alpha': 0.52}\n",
      "0.8456 for {'alpha': 0.53}\n",
      "0.8456 for {'alpha': 0.54}\n",
      "0.8455 for {'alpha': 0.55}\n",
      "0.8456 for {'alpha': 0.56}\n",
      "0.8456 for {'alpha': 0.5700000000000001}\n",
      "0.8456 for {'alpha': 0.5800000000000001}\n",
      "0.8456 for {'alpha': 0.59}\n",
      "0.8456 for {'alpha': 0.6}\n",
      "0.8457 for {'alpha': 0.61}\n",
      "0.8457 for {'alpha': 0.62}\n",
      "0.8457 for {'alpha': 0.63}\n",
      "0.8456 for {'alpha': 0.64}\n",
      "0.8456 for {'alpha': 0.65}\n",
      "0.8455 for {'alpha': 0.66}\n",
      "0.8455 for {'alpha': 0.67}\n",
      "0.8455 for {'alpha': 0.68}\n",
      "0.8455 for {'alpha': 0.6900000000000001}\n",
      "0.8455 for {'alpha': 0.7000000000000001}\n",
      "0.8455 for {'alpha': 0.7100000000000001}\n",
      "0.8455 for {'alpha': 0.72}\n",
      "0.8456 for {'alpha': 0.73}\n",
      "0.8455 for {'alpha': 0.74}\n",
      "0.8455 for {'alpha': 0.75}\n",
      "0.8455 for {'alpha': 0.76}\n",
      "0.8453 for {'alpha': 0.77}\n",
      "0.8453 for {'alpha': 0.78}\n",
      "0.8454 for {'alpha': 0.79}\n",
      "0.8455 for {'alpha': 0.8}\n",
      "0.8455 for {'alpha': 0.81}\n",
      "0.8454 for {'alpha': 0.8200000000000001}\n",
      "0.8454 for {'alpha': 0.8300000000000001}\n",
      "0.8453 for {'alpha': 0.8400000000000001}\n",
      "0.8452 for {'alpha': 0.85}\n",
      "0.8452 for {'alpha': 0.86}\n",
      "0.8450 for {'alpha': 0.87}\n",
      "0.8451 for {'alpha': 0.88}\n",
      "0.8450 for {'alpha': 0.89}\n",
      "0.8450 for {'alpha': 0.9}\n",
      "0.8450 for {'alpha': 0.91}\n",
      "0.8450 for {'alpha': 0.92}\n",
      "0.8450 for {'alpha': 0.93}\n",
      "0.8450 for {'alpha': 0.9400000000000001}\n",
      "0.8449 for {'alpha': 0.9500000000000001}\n",
      "0.8450 for {'alpha': 0.9600000000000001}\n",
      "0.8449 for {'alpha': 0.97}\n",
      "0.8449 for {'alpha': 0.98}\n",
      "0.8450 for {'alpha': 0.99}\n",
      "0.8450 for {'alpha': 1.0}\n",
      "\n",
      "Optimal Parameters: {'alpha': 0.060000000000000005}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.8705\n",
      "VALID: 0.8694\n",
      "TEST: 0.8419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    7.9s finished\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Bernoulli Naive Bayes', IMDB_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 48 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=2)]: Done  48 out of  48 | elapsed:   26.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREES\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.7154 for {'max_depth': 13, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.7212 for {'max_depth': 13, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.7125 for {'max_depth': 13, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.7215 for {'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.7240 for {'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.7241 for {'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.7225 for {'max_depth': 13, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.7184 for {'max_depth': 13, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.7259 for {'max_depth': 13, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.7214 for {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.7195 for {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.7210 for {'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "0.7060 for {'max_depth': 14, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.7031 for {'max_depth': 14, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.7240 for {'max_depth': 14, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.7246 for {'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.7263 for {'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.6989 for {'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.7205 for {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.7238 for {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.7330 for {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.7203 for {'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.7241 for {'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.7207 for {'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "0.7114 for {'max_depth': 15, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.6924 for {'max_depth': 15, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.6629 for {'max_depth': 15, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.7107 for {'max_depth': 15, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.7195 for {'max_depth': 15, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.7160 for {'max_depth': 15, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.7215 for {'max_depth': 15, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.7162 for {'max_depth': 15, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.7267 for {'max_depth': 15, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.7308 for {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.7197 for {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.7267 for {'max_depth': 15, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "0.6938 for {'max_depth': 16, 'max_features': 0.1, 'min_samples_leaf': 3}\n",
      "0.7058 for {'max_depth': 16, 'max_features': 0.1, 'min_samples_leaf': 4}\n",
      "0.7154 for {'max_depth': 16, 'max_features': 0.1, 'min_samples_leaf': 5}\n",
      "0.7288 for {'max_depth': 16, 'max_features': 0.2, 'min_samples_leaf': 3}\n",
      "0.7245 for {'max_depth': 16, 'max_features': 0.2, 'min_samples_leaf': 4}\n",
      "0.7245 for {'max_depth': 16, 'max_features': 0.2, 'min_samples_leaf': 5}\n",
      "0.7276 for {'max_depth': 16, 'max_features': 0.30000000000000004, 'min_samples_leaf': 3}\n",
      "0.7272 for {'max_depth': 16, 'max_features': 0.30000000000000004, 'min_samples_leaf': 4}\n",
      "0.7242 for {'max_depth': 16, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "0.7174 for {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 3}\n",
      "0.7238 for {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 4}\n",
      "0.7244 for {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 5}\n",
      "\n",
      "Optimal Parameters: {'max_depth': 14, 'max_features': 0.30000000000000004, 'min_samples_leaf': 5}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.7721\n",
      "VALID: 0.7684\n",
      "TEST: 0.7330\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Decision Trees', IMDB_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 80 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=2)]: Done  57 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=2)]: Done  80 out of  80 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR SVM\n",
      "\n",
      "Hypertuning Parameters:\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1000}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1100}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1200}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1300}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1400}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1500}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1600}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1700}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1800}\n",
      "0.8784 for {'C': 0.01, 'max_iter': 1900}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1000}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1100}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1200}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1300}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1400}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1500}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1600}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1700}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1800}\n",
      "0.8681 for {'C': 0.0372759372031494, 'max_iter': 1900}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1000}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1100}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1200}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1300}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1400}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1500}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1600}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1700}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1800}\n",
      "0.8567 for {'C': 0.13894954943731375, 'max_iter': 1900}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1000}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1100}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1200}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1300}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1400}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1500}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1600}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1700}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1800}\n",
      "0.8466 for {'C': 0.517947467923121, 'max_iter': 1900}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1000}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1100}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1200}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1300}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1400}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1500}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1600}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1700}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1800}\n",
      "0.8437 for {'C': 1.9306977288832496, 'max_iter': 1900}\n",
      "0.8416 for {'C': 7.196856730011514, 'max_iter': 1000}\n",
      "0.8414 for {'C': 7.196856730011514, 'max_iter': 1100}\n",
      "0.8414 for {'C': 7.196856730011514, 'max_iter': 1200}\n",
      "0.8415 for {'C': 7.196856730011514, 'max_iter': 1300}\n",
      "0.8414 for {'C': 7.196856730011514, 'max_iter': 1400}\n",
      "0.8415 for {'C': 7.196856730011514, 'max_iter': 1500}\n",
      "0.8418 for {'C': 7.196856730011514, 'max_iter': 1600}\n",
      "0.8414 for {'C': 7.196856730011514, 'max_iter': 1700}\n",
      "0.8415 for {'C': 7.196856730011514, 'max_iter': 1800}\n",
      "0.8417 for {'C': 7.196856730011514, 'max_iter': 1900}\n",
      "0.8411 for {'C': 26.826957952797247, 'max_iter': 1000}\n",
      "0.8407 for {'C': 26.826957952797247, 'max_iter': 1100}\n",
      "0.8408 for {'C': 26.826957952797247, 'max_iter': 1200}\n",
      "0.8409 for {'C': 26.826957952797247, 'max_iter': 1300}\n",
      "0.8410 for {'C': 26.826957952797247, 'max_iter': 1400}\n",
      "0.8406 for {'C': 26.826957952797247, 'max_iter': 1500}\n",
      "0.8407 for {'C': 26.826957952797247, 'max_iter': 1600}\n",
      "0.8412 for {'C': 26.826957952797247, 'max_iter': 1700}\n",
      "0.8409 for {'C': 26.826957952797247, 'max_iter': 1800}\n",
      "0.8408 for {'C': 26.826957952797247, 'max_iter': 1900}\n",
      "0.8407 for {'C': 100.0, 'max_iter': 1000}\n",
      "0.8407 for {'C': 100.0, 'max_iter': 1100}\n",
      "0.8406 for {'C': 100.0, 'max_iter': 1200}\n",
      "0.8409 for {'C': 100.0, 'max_iter': 1300}\n",
      "0.8406 for {'C': 100.0, 'max_iter': 1400}\n",
      "0.8405 for {'C': 100.0, 'max_iter': 1500}\n",
      "0.8406 for {'C': 100.0, 'max_iter': 1600}\n",
      "0.8404 for {'C': 100.0, 'max_iter': 1700}\n",
      "0.8406 for {'C': 100.0, 'max_iter': 1800}\n",
      "0.8408 for {'C': 100.0, 'max_iter': 1900}\n",
      "\n",
      "Optimal Parameters: {'C': 0.01, 'max_iter': 1000}\n",
      "Corresponding F1 Scores:\n",
      "TRAIN: 0.9532\n",
      "VALID: 0.9516\n",
      "TEST: 0.8774\n"
     ]
    }
   ],
   "source": [
    "classifier(models, 'Linear SVM', IMDB_BBOW, tune=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
